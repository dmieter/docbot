import os
import telebot
from langchain.prompts import PromptTemplate
import chromadb
from langchain.vectorstores import Chroma
from datetime import datetime
from telebot import types
import doc_core as dc
import yaml
from data import list_mpei_documents as lmd

#BOT_TOKEN = os.environ.get('DOCBOT_TOKEN')
#bot = telebot.TeleBot(BOT_TOKEN)
bot = None

config = {}
answer_chains = {}
retrievers = {}
chat_history = {}

def load_config():
  global config
  
  with open('config/config_dpo.yml', 'r') as file:
      config = yaml.safe_load(file)
      print("config reloaded: {}".format(config))



def init():
  load_config()

  global bot
  bot = telebot.TeleBot(config['bot_token'])
  os.environ.setdefault("GIGACHAT_CREDENTIALS", config['giga_creds'])

  answer_chains.clear()
  retrievers.clear()
  
  for category_name in config['categories'].keys():
    category = config['categories'][category_name]

    prompt_template_name = get_config_value('prompt', category, config['default'])
    prompt_template = dc.prompts[prompt_template_name]
    prompt = PromptTemplate.from_template(prompt_template)

    display_name = category['display_name']
    db_path = get_config_value('db_path', category, config['default'])
    retriever_config = get_config_value('retriever', category, config['default'])

    answer_chain, retriever = dc.prepareAnswerChain(db_path, category_name, dc.embeddings, dc.llm, prompt, search_args=retriever_config)
                                                    #search_args={"score_threshold": retriever['threshold'], "k": retriever['k'], "s" : retriever['s'], "t" : retriever['t'], "neighbours": [1500, 1000]})
    
    if answer_chain:
      answer_chains[display_name] = answer_chain
      retrievers[display_name] = retriever


init()

@bot.message_handler(commands=['reload'])
def reload(message):
  init()
  bot.reply_to(message, "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –æ–±–Ω–æ–≤–ª–µ–Ω–∞")

@bot.message_handler(commands=['start'])
def start(message):
  chat_history.clear()
  bot.send_message(message.from_user.id, """üññ –ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∞–º –î–ü–û! –±—É–¥—É —Ä–∞–¥ –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–∞—à–∏ –≤–æ–ø—Ä–æ—Å—ã.""")


DPO_LINKS = {'–≠–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Å—Ç–∞–Ω—Ü–∏–∏' : 'https://mpei.ru/Education/educationalprograms/2023/Lists/DopPrograms2023/disp.aspx?ID=982',
             '–≠–ª–µ–∫—Ç—Ä–æ—Å–Ω–∞–±–∂–µ–Ω–∏–µ' : 'https://mpei.ru/Education/educationalprograms/2023/Lists/DopPrograms2023/disp.aspx?ID=1199',
             '–ü—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ–µ –∏ –≥—Ä–∞–∂–¥–∞–Ω—Å–∫–æ–µ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ' : 'https://mpei.ru/Education/educationalprograms/2023/Lists/DopPrograms2023/disp.aspx?ID=1026',
             '–ú–µ–Ω–µ–¥–∂–º–µ–Ω—Ç –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö, –º—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω—ã—Ö –∏ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –∑–∞–∫—É–ø–æ–∫' : 'https://mpei.ru/Education/educationalprograms/2023/Lists/DopPrograms2023/disp.aspx?ID=1309',
             '–ú–µ–Ω–µ–¥–∂–º–µ–Ω—Ç –∑–∞–∫—É–ø–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤, —Ä–∞–±–æ—Ç, —É—Å–ª—É–≥' : 'https://mpei.ru/Education/educationalprograms/2023/Lists/DopPrograms2023/disp.aspx?ID=1315',
             '–û–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ, –Ω–∞–ª–∞–¥–∫–∞, –º–∞—Ç–µ—Ä–∏–∞–ª—ã –∏ –º–µ—Ç–æ–¥—ã –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞' : 'https://mpei.ru/Education/educationalprograms/2023/Lists/DopPrograms2023/disp.aspx?ID=1518',
             '–û—Ü–µ–Ω–∫–∞ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è' : 'https://mpei.ru/Education/educationalprograms/2023/Lists/DopPrograms2023/disp.aspx?ID=1284',
             '–ü–µ—Ä–µ–≤–æ–¥—á–∏–∫ –≤ —Å—Ñ–µ—Ä–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏' : 'https://mpei.ru/Education/educationalprograms/2023/Lists/DopPrograms2023/disp.aspx?ID=1068',
             '–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–º–µ—Ç–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏':  'https://mpei.ru/Education/educationalprograms/2023/Lists/DopPrograms2023/disp.aspx?ID=1319',
             '–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∏ –º—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω—ã–º–∏ –∑–∞–∫—É–ø–∫–∞–º–∏' : 'https://mpei.ru/Education/educationalprograms/2023/Lists/DopPrograms2023/disp.aspx?ID=1176',
             '–£–ª—å—Ç—Ä–∞–∑–≤—É–∫–æ–≤–æ–π –∫–æ–Ω—Ç—Ä–æ–ª—å —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º —Å–∏—Å—Ç–µ–º—ã –Ω–∞ —Ñ–∞–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ—à—ë—Ç–∫–∞—Ö HARFANG VEO' : 'https://mpei.ru/Education/educationalprograms/2023/Lists/DopPrograms2023/disp.aspx?ID=1175',
             '–≠–ª–µ–∫—Ç—Ä–æ—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞ –∏ —ç–ª–µ–∫—Ç—Ä–æ—Ç–µ—Ö–Ω–∏–∫–∞' : 'https://mpei.ru/Education/educationalprograms/2023/Lists/DopPrograms2023/disp.aspx?ID=1444',
             '–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ —Å–≤–æ–π—Å—Ç–≤ –∫—Ä–∏—Å—Ç–∞–ª–ª–∏—á–µ—Å–∫–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤' : 'https://mpei.ru/Education/educationalprograms/2023/Lists/DopPrograms2023/disp.aspx?ID=996'}
def prepare_dpo_links(answer):
  references_str = ""

  for key, value in DPO_LINKS.items():
    if key.lower() in answer.lower():
      if not references_str:
        references_str = "\n–ü–æ–¥—Ä–æ–±–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ –ø–æ —Å–æ–æ—Ç–≤–µ—Ç—Å–≤—É—é—â–∏–º –ø—Ä–æ–≥—Ä–∞–º–º–∞–º:\n"
      references_str += "<a href='{}'>{}</a>\n".format(value, key)

  return references_str
  

def prepare_links(answer, docs):
  references_str = "\n"
  references = set()

  for doc in docs:
    if "url" in doc.metadata.keys() and doc.metadata["url"]:
      #references.add(doc.metadata["url"])
      references.add("<a href='{}'>{}</a>".format(doc.metadata["url"], doc.metadata["name"]))

  if len(references) > 0:
    references_str += '\n'.join(list(references))

  references_str += prepare_dpo_links(answer)

  return references_str

def prepare_answer(question, knowledge):

  if knowledge in answer_chains.keys():
    answer = answer_chains[knowledge].invoke(question)

    related_docs = retrievers[knowledge].get_relevant_documents(question)
    answer += prepare_links(answer, related_docs)
    #print("Retrieved docs: {}".format(related_docs))

  else:
    answer = "–ß—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫. \n–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å –±–æ—Ç–∞ —á–µ—Ä–µ–∑ –∫–æ–º–∞–Ω–¥—É /start".format(knowledge)

  return answer


def trim_question(question):
  return trim_text(question, 800)

def trim_text(text, size):
  return text[:size] if len(text) > size else text

def is_additional_question(question):
  keywords = ["–∞", "–∏", "–Ω—É", "—Ç–µ–ø–µ—Ä—å", "—Ç–æ–ª—å–∫–æ", "—Ö–æ—Ä–æ—à–æ", "—Ö–æ—Ä–æ—à–æ,", "–Ω–µ—Ç", "–Ω–µ—Ç,"]
  trimmed_question = question.strip().lower()
  if len(trimmed_question) > 0:
    first_word = trimmed_question.split()[0]
    return first_word in keywords or trimmed_question[0] == '>'
  else:
    return False
  
def prepare_question_chain(message):
  global chat_history

  prev_messages = []
  if message.chat.id in chat_history.keys():
    prev_messages = chat_history[message.chat.id]

  trimmed_question = trim_question(message.text)

  # if no history or question isnt additional then just create a new question thread and return current question
  if len(prev_messages) == 0 or not is_additional_question(trimmed_question):
    chat_history[message.chat.id] = [trimmed_question]
    return trimmed_question
  
  # else need to retrieve previous messages as context and add new question to the end
  thread = '\n'.join(prev_messages)
  chat_history[message.chat.id].append(trimmed_question)

  return '{} \n {}'.format(thread, trimmed_question)
  


@bot.message_handler(func=lambda msg: True)
def general_question(message):

  chat = bot.get_chat(message.chat.id)
  question = message.text
  print(">>>>>>>>>>>>>>> QQQ: " + str(datetime.now()) + " " + str(message.chat.id) + ": " + question)

  question_with_context = prepare_question_chain(message)
  print("Question with context: {}".format(question_with_context))

  knowledge_base = config['default']['knowledge_base']
  answer = prepare_answer(question_with_context, knowledge_base).replace('_', ' ')
  
  print(">>>>>>>>>>>>>>> AAA " + str(message.chat.id) + ": " + answer)
  suffix = """\n<b>{}.</b>""".format(knowledge_base)
  #bot.reply_to(message, answer + suffix, parse_mode = 'Markdown')
  bot.reply_to(message, trim_text(answer, 3500) + suffix, parse_mode = 'HTML')

#init()
print(str(datetime.now()) + " ChatDPO is here!")
bot.infinity_polling()